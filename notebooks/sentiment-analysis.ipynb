{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# General settings\n",
    "start_time = time.time()\n",
    "color = sns.color_palette()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName('Yelp! Sentiment Analysis')\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = spark.read.json(\"gs://ca4022_yelp_data/data_raw/yelp_academic_dataset_review.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "      <td>0</td>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>5</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "      <td>1</td>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>1</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "      <td>0</td>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>1</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "      <td>0</td>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>0</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "      <td>0</td>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nlxHRv1zXGT0c0K51q3jDg</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-07 07:25:25</td>\n",
       "      <td>0</td>\n",
       "      <td>L2O_INwlrRuoX05KSjc4eg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is definitely my favorite fast food sub s...</td>\n",
       "      <td>2</td>\n",
       "      <td>5vD2kmE25YBrbayKhykNxQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pthe4qk5xh4n-ef-9bvMSg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-05 23:11:05</td>\n",
       "      <td>0</td>\n",
       "      <td>ZayJ1zWyWgY9S_TRLT_y9Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Really good place with simple decor, amazing f...</td>\n",
       "      <td>1</td>\n",
       "      <td>aq_ZxGHiri48TUXJlpRkCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FNCJpSn0tL9iqoY3JC73qw</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-18 18:31:54</td>\n",
       "      <td>0</td>\n",
       "      <td>lpFIJYpsvDxyph-kPzZ6aA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome office and staff, very professional an...</td>\n",
       "      <td>0</td>\n",
       "      <td>dsd-KNYKMpx6ma_sRWCSkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e_BiI4ej1CW1F0EyVLr-FQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-16 06:48:47</td>\n",
       "      <td>0</td>\n",
       "      <td>JA-xnyHytKiOIHl_ztnK9Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Most delicious authentic Italian I've had in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>P6apihD4ASf1vpPxHODxAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ws8V970-mQt2X9CwCuT5zw</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-10-13 04:16:41</td>\n",
       "      <td>0</td>\n",
       "      <td>z4BCgTkfNtCu4XY5Lp97ww</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I have been here twice. Very nice and laid bac...</td>\n",
       "      <td>3</td>\n",
       "      <td>jOERvhmK6_lo_XGUBPws_w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  -MhfebM0QIsKt87iDN-FNw     0  2015-04-15 05:21:16      0   \n",
       "1  lbrU8StCq3yDfr-QMnGrmQ     0  2013-12-07 03:16:52      1   \n",
       "2  HQl28KMwrEKHqhFrrDqVNQ     0  2015-12-05 03:18:11      0   \n",
       "3  5JxlZaqCnk1MnbgRirs40Q     0  2011-05-27 05:30:52      0   \n",
       "4  IS4cv902ykd8wj1TR0N3-A     0  2017-01-14 21:56:57      0   \n",
       "5  nlxHRv1zXGT0c0K51q3jDg     0  2013-05-07 07:25:25      0   \n",
       "6  Pthe4qk5xh4n-ef-9bvMSg     0  2015-11-05 23:11:05      0   \n",
       "7  FNCJpSn0tL9iqoY3JC73qw     0  2017-07-18 18:31:54      0   \n",
       "8  e_BiI4ej1CW1F0EyVLr-FQ     0  2015-02-16 06:48:47      0   \n",
       "9  Ws8V970-mQt2X9CwCuT5zw     1  2009-10-13 04:16:41      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ    2.0   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA    1.0   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw    5.0   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw    1.0   \n",
       "4  6TdNDKywdbjoTkizeMce8A    4.0   \n",
       "5  L2O_INwlrRuoX05KSjc4eg    5.0   \n",
       "6  ZayJ1zWyWgY9S_TRLT_y9Q    5.0   \n",
       "7  lpFIJYpsvDxyph-kPzZ6aA    5.0   \n",
       "8  JA-xnyHytKiOIHl_ztnK9Q    5.0   \n",
       "9  z4BCgTkfNtCu4XY5Lp97ww    4.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  As someone who has worked with many museums, I...       5   \n",
       "1  I am actually horrified this place is still in...       1   \n",
       "2  I love Deagan's. I do. I really do. The atmosp...       1   \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...       0   \n",
       "4  Oh happy day, finally have a Canes near my cas...       0   \n",
       "5  This is definitely my favorite fast food sub s...       2   \n",
       "6  Really good place with simple decor, amazing f...       1   \n",
       "7  Awesome office and staff, very professional an...       0   \n",
       "8  Most delicious authentic Italian I've had in t...       0   \n",
       "9  I have been here twice. Very nice and laid bac...       3   \n",
       "\n",
       "                  user_id  \n",
       "0  OwjRMXRC0KyPrIlcjaXeFQ  \n",
       "1  nIJD_7ZXHq-FX8byPMOkMQ  \n",
       "2  V34qejxNsCbcgD8C0HVk-Q  \n",
       "3  ofKDkJKXSKZXu5xJNGiiBQ  \n",
       "4  UgMW8bLE0QMJDCkQ1Ax5Mg  \n",
       "5  5vD2kmE25YBrbayKhykNxQ  \n",
       "6  aq_ZxGHiri48TUXJlpRkCQ  \n",
       "7  dsd-KNYKMpx6ma_sRWCSkQ  \n",
       "8  P6apihD4ASf1vpPxHODxAQ  \n",
       "9  jOERvhmK6_lo_XGUBPws_w  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to carry out sentiment analysis on reviews, we need to frame the problem. In this instance, we are assuming that the sentiment of the review is reflected by the star rating given, which should be the case naturally.\n",
    "\n",
    "Therefore, for an unseen review, we want to be able to predict what the sentiment of the review is and thus, the star rating. To simplify the problem, we assume:\n",
    "* A review will have a **positive** sentiment if a star rating of [4.0, 4.5, 5.0] is present.\n",
    "* A review will have a **neutral** sentiment if a star rating of [2,5, 3.0, 3.5] is present.\n",
    "* A review will have a **negative** sentiment if a star rating of [1.0, 1.5, 2.0] is present.\n",
    "\n",
    "These sentiments will be denoted as follows:\n",
    "* **positive** $\\Leftrightarrow$ 1\n",
    "* **neutral** $\\Leftrightarrow$ 0\n",
    "* **negative** $\\Leftrightarrow$ -1\n",
    "\n",
    "In order to prepare the data for this experiment, we simply need the review itself and then a sentiment score based on the star rating present. Essentially, the question which we are seeking to answer is: *is the star rating associated with a review an accurate representation of the sentiment of the review itself?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = df_review.select([\"text\", \"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = df_sent.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is definitely my favorite fast food sub s...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Really good place with simple decor, amazing f...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Awesome office and staff, very professional an...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Most delicious authentic Italian I've had in t...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have been here twice. Very nice and laid bac...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  As someone who has worked with many museums, I...    2.0\n",
       "1  I am actually horrified this place is still in...    1.0\n",
       "2  I love Deagan's. I do. I really do. The atmosp...    5.0\n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...    1.0\n",
       "4  Oh happy day, finally have a Canes near my cas...    4.0\n",
       "5  This is definitely my favorite fast food sub s...    5.0\n",
       "6  Really good place with simple decor, amazing f...    5.0\n",
       "7  Awesome office and staff, very professional an...    5.0\n",
       "8  Most delicious authentic Italian I've had in t...    5.0\n",
       "9  I have been here twice. Very nice and laid bac...    4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a function to convert the star ratings to sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def stars_to_sent(rating):\n",
    "    if rating > 3.5:\n",
    "        return 1\n",
    "    elif rating < 2.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "udfStarsToSent = udf(stars_to_sent, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = df_sent.withColumn(\"sentiment\", udfStarsToSent(\"stars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is definitely my favorite fast food sub s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Really good place with simple decor, amazing f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Awesome office and staff, very professional an...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Most delicious authentic Italian I've had in t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have been here twice. Very nice and laid bac...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  sentiment\n",
       "0  As someone who has worked with many museums, I...    2.0         -1\n",
       "1  I am actually horrified this place is still in...    1.0         -1\n",
       "2  I love Deagan's. I do. I really do. The atmosp...    5.0          1\n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...    1.0         -1\n",
       "4  Oh happy day, finally have a Canes near my cas...    4.0          1\n",
       "5  This is definitely my favorite fast food sub s...    5.0          1\n",
       "6  Really good place with simple decor, amazing f...    5.0          1\n",
       "7  Awesome office and staff, very professional an...    5.0          1\n",
       "8  Most delicious authentic Italian I've had in t...    5.0          1\n",
       "9  I have been here twice. Very nice and laid bac...    4.0          1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the masssive size of the dataset (over 8 million reviews), and the long training time this would result in for the sentiment analysis task, we decided to take a random sample of 20% from the original dataset. The data is then split into the train, validation and test sets. Due to the fact that the dataset is still so large, we only require 1% of the data for validation and a further 1% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random 20% sample\n",
    "df_samp = df_sent.sample(True, 0.2, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df_samp.randomSplit([0.98, 0.01, 0.01], seed = 445)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of each set is shown below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1571827\n",
      "Validation:  16189\n",
      "Test:  15884\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: \", train_set.count())\n",
    "print(\"Validation: \", val_set.count())\n",
    "print(\"Test: \", test_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF for tf-idf Calculation with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of research, we identified a number of approaches which can be used to perform sentiment analysis. One of which is utilisng tf-idf with a Logistic Regression model. This involves:\n",
    "1. Tokenising the review text.\n",
    "2. Using a Hashing term frequency (tf) function to calculate the frequency of terms.\n",
    "3. Computing the inverse document frequency (idf) of each term.\n",
    "4. Mapping the sentiment labels.\n",
    "\n",
    "Spark has a useful Pipeline functionality which enables us to chain these processing steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!! STAY AWAY!!!!\\nTheir name says it all as i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[!!!, stay, away!!!!, their, name, says, it, a...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A Night at the Roxbury\" of Japanese restauran...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"a, night, at, the, roxbury\", of, japanese, r...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"All of THAT ...and a bag of chips\"!!\\nThis pl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"all, of, that, ...and, a, bag, of, chips\"!!,...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Authority\" keeps guard here, so this must be ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[\"authority\", keeps, guard, here,, so, this, m...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Each chicken, feed some people\"- Their slogan...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"each, chicken,, feed, some, people\"-, their,...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Ew! Alan, did you just eat sofa pizza?\"\\n-Stu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"ew!, alan,, did, you, just, eat, sofa, pizza...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"First timers!\" Multiple servers yelled as the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"first, timers!\", multiple, servers, yelled, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Fun atmosphere, good cocktails, and yummy smo...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"fun, atmosphere,, good, cocktails,, and, yum...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Hey! Hey! Hey!\\n\"I don't like walking around ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"hey!, hey!, hey!, \"i, don't, like, walking, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Holy cow, this man just gave me 'f**k me hair...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"holy, cow,, this, man, just, gave, me, 'f**k...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  sentiment  \\\n",
       "0  !!! STAY AWAY!!!!\\nTheir name says it all as i...    1.0         -1   \n",
       "1  \"A Night at the Roxbury\" of Japanese restauran...    2.0         -1   \n",
       "2  \"All of THAT ...and a bag of chips\"!!\\nThis pl...    5.0          1   \n",
       "3  \"Authority\" keeps guard here, so this must be ...    1.0         -1   \n",
       "4  \"Each chicken, feed some people\"- Their slogan...    5.0          1   \n",
       "5  \"Ew! Alan, did you just eat sofa pizza?\"\\n-Stu...    5.0          1   \n",
       "6  \"First timers!\" Multiple servers yelled as the...    5.0          1   \n",
       "7  \"Fun atmosphere, good cocktails, and yummy smo...    4.0          1   \n",
       "8  \"Hey! Hey! Hey!\\n\"I don't like walking around ...    3.0          0   \n",
       "9  \"Holy cow, this man just gave me 'f**k me hair...    5.0          1   \n",
       "\n",
       "                                               words  \\\n",
       "0  [!!!, stay, away!!!!, their, name, says, it, a...   \n",
       "1  [\"a, night, at, the, roxbury\", of, japanese, r...   \n",
       "2  [\"all, of, that, ...and, a, bag, of, chips\"!!,...   \n",
       "3  [\"authority\", keeps, guard, here,, so, this, m...   \n",
       "4  [\"each, chicken,, feed, some, people\"-, their,...   \n",
       "5  [\"ew!, alan,, did, you, just, eat, sofa, pizza...   \n",
       "6  [\"first, timers!\", multiple, servers, yelled, ...   \n",
       "7  [\"fun, atmosphere,, good, cocktails,, and, yum...   \n",
       "8  [\"hey!, hey!, hey!, \"i, don't, like, walking, ...   \n",
       "9  [\"holy, cow,, this, man, just, gave, me, 'f**k...   \n",
       "\n",
       "                                                  tf  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            features  label  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    1.0  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
       "5  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
       "6  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
       "7  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  \n",
       "8  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    2.0  \n",
       "9  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol=\"tf\")\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample processed reviews are shown above. Note however that these are not representative of how the tf and idf values should look due to how the DataFrame is sorted (i.e. exclamation marks and quotations come first alphabetically).\n",
    "\n",
    "The next step is to train our Logistic Regression model and see how it fares in predicting the sentiment of unseen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070288452310928"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=50)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Spark returns the area under ROC curve. A simpler evaluation metric is accuracy which is calculated below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8624374575328927"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, based on the unseen reviews in the test set, the classifier predicts the sentiment of these reviews with an accuracy of 86%, which is very good! This gives us an early indication that the stars ratings correlate well with the sentiment of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer for tf-idf Calculation with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf can also be calculated in another manner using the CountVectorizer, available in the SparkML package. The CountVectorizer differs from the HashingTF approach in how it filters the features. HashingTF employs a dimensionality reduction technique which allows the possibility of collisons. Conversely, the CountVectorizer approach discards infrequent tokens. \n",
    "\n",
    "Below, we evaluate the performance of Logistic Regression using the CountVectorizer approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8646\n",
      "ROC-AUC: 0.9085\n",
      "CPU times: user 761 ms, sys: 305 ms, total: 1.07 s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
    "\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that the CountVectorizer approach has marginally improved the performance of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of embedding the text from the reviews is through the use of n-grams. The N-gram implementation is slightly more difficult through Spark than with scikit-learn. Originally, approximately 16,000 features for each of unigram, bigram and trigram were extracted (A VectorAssembler is used in the pipeline to combine these features). Then, these features were reduced using Chi-Squared feature selection. However, the training time for this approach was far too long.\n",
    "\n",
    "As a result, we decided to extract 5,460 features each from unigram, bigram and trigram firstly, and then we would have 16,000 features without having to perform the cumbersome Chi-Squared feature selection. Using this approach, training time dropped drastically but similar accuracy was achieved, making it a preferable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "\n",
    "def build_ngrams_wocs(inputCol=[\"text\",\"sentiment\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8784\n",
      "ROC-AUC: 0.9265\n",
      "CPU times: user 1.07 s, sys: 497 ms, total: 1.56 s\n",
      "Wall time: 38min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
    "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
    "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
    "roc_auc_wocs = evaluator.evaluate(predictions_wocs)\n",
    "\n",
    "# print accuracy, roc_auc\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, again, slightly better accuracy than the previous algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to run the algorithm with the highest accuracy on the test set to get a final accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8806\n",
      "ROC-AUC: 0.9291\n"
     ]
    }
   ],
   "source": [
    "test_predictions = trigramwocs_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# print accuracy, roc_auc\n",
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy result on the test set was 88.06% which is very good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}